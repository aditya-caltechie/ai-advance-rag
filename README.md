# Advanced RAG Pipeline - Implementation & Evaluation

This repository demonstrates the evolution from **Basic RAG** to **Advanced RAG** with comprehensive evaluation metrics.

## ğŸ¯ Repository Overview

This project contains:
1. **Basic RAG Implementation** (`implementation/`) - Traditional RAG pipeline
2. **Advanced RAG Implementation** (`pro_implementation/`) - Enhanced with query rewriting, dual retrieval, and LLM reranking
3. **Evaluation Framework** - Measure retrieval quality (MRR, nDCG) and answer quality (LLM-as-judge)
4. **Comparative Analysis** - Documentation showing improvements from advanced techniques

## ğŸ“Š Project Workflow

**Visual Journey**: See the [Complete RAG Evolution Journey](docs/complete_journey.md) for a visual roadmap

### Phase 1: Basic RAG Implementation

```mermaid
graph TD
    A[1. Run implementation/ingest.py] --> B[Build Vector DB with Basic Chunking]
    B --> C[2. Run app.py with implementation]
    C --> D[Test Basic RAG System]
    D --> E[3. Run evaluator.py with implementation]
    E --> F[Get Baseline Metrics<br/>MRR, nDCG, Accuracy]
    
    style F fill:#ffeb99
```

**Baseline Results**: Establishes initial performance metrics (~75% retrieval, ~3.8/5 answer quality)

### Phase 2: Advanced RAG Implementation

```mermaid
graph TD
    A[1. Run pro_implementation/ingest.py] --> B[Build Vector DB with LLM Chunking<br/>+ Headlines + Summaries]
    B --> C[2. Run app.py with pro_implementation]
    C --> D[Test Advanced RAG System<br/>Query Rewrite + Dual Retrieval + Reranking]
    D --> E[3. Run evaluator.py with pro_implementation]
    E --> F[Get Improved Metrics<br/>Higher MRR, nDCG, Accuracy]
    
    style F fill:#90ee90
```

**Expected Outcome**: 15-25% improvement in retrieval and answer quality (~92% retrieval, ~4.6/5 answer quality)

---

## ğŸ—ï¸ Architecture Comparison

### Basic RAG Architecture

See detailed architecture: [Basic RAG Architecture](docs/basic_rag_architecture.md)

**Key Characteristics:**
- Simple rule-based chunking (500 chars, 200 overlap)
- Direct vector similarity search (K=10)
- Single query, no optimization
- Fast (1-2s) but less accurate (70-80%)

### Advanced RAG Architecture  

See detailed architecture: [Advanced RAG Architecture](docs/advanced_rag_architecture.md)

**Key Characteristics:**
- LLM-powered semantic chunking with metadata
- Query rewriting + dual retrieval (K=20 each)
- LLM reranking â†’ top K=10
- Slower (3-5s) but more accurate (85-95%)

**Visual Comparison:**

| Phase | Basic RAG | Advanced RAG |
|-------|-----------|--------------|
| **Chunking** | Fixed character splits | LLM semantic splits + headlines |
| **Retrieval** | 1 query â†’ similarity search | 2 queries (original + rewritten) |
| **Ranking** | Cosine similarity only | LLM semantic reranking |
| **Context** | Plain text chunks | Enhanced chunks with sources |
| **LLM Calls** | 1 (answer generation) | 4 (chunk, rewrite, rerank, answer) |

---

## ğŸ“ Repository Structure

```
ai-advance-rag/
â”œâ”€â”€ src/rag-pipeline/
â”‚   â”œâ”€â”€ implementation/          # Basic RAG
â”‚   â”‚   â”œâ”€â”€ ingest.py           # Simple text chunking
â”‚   â”‚   â””â”€â”€ answer.py           # Single-query retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ pro_implementation/      # Advanced RAG
â”‚   â”‚   â”œâ”€â”€ ingest.py           # LLM-powered chunking
â”‚   â”‚   â””â”€â”€ answer.py           # Multi-stage retrieval
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/              # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ eval.py             # Metrics calculation
â”‚   â”‚   â””â”€â”€ test.py             # Test cases
â”‚   â”‚
â”‚   â”œâ”€â”€ app.py                   # Gradio UI (switch imports)
â”‚   â””â”€â”€ evaluator.py             # Evaluation dashboard
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ rag_vs_pro.md           # Detailed comparison
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

**For detailed step-by-step instructions, see the [Complete Workflow Guide](docs/workflow_guide.md)**

### Step 1: Setup Environment

```bash
# Install dependencies using uv
uv sync

# Configure API keys in .env (copy from .env.example)
cp .env.example .env
# Edit .env and add your API keys
```

### Step 2: Run Basic RAG

```bash
# 1. Ingest documents (basic chunking)
cd src/rag-pipeline
uv run implementation/ingest.py

# 2. Launch UI
uv run app.py  # Uses implementation/ by default

# 3. Evaluate baseline (update imports in evaluator.py to use implementation/)
uv run evaluator.py
```

**ğŸ“¸ Screenshot your baseline metrics!**

### Step 3: Run Advanced RAG

```bash
# 1. Ingest with advanced chunking
uv run pro_implementation/ingest.py

# 2. Update app.py imports:
# Change: from implementation.answer import answer_question
# To: from pro_implementation.answer import answer_question

# 3. Launch UI
uv run app.py

# 4. Evaluate improvements (update imports to use pro_implementation/)
uv run evaluator.py
```

**ğŸ“¸ Screenshot improved metrics and compare!**

**Expected Improvements:** 15-25% boost in MRR, nDCG, and answer quality scores.

---

## ğŸ“ˆ Key Improvements in Advanced RAG

| Technique | Implementation | Benefit |
|-----------|---------------|---------|
| **LLM Chunking** | GPT generates semantic chunks with headlines + summaries | Better retrieval accuracy |
| **Query Rewriting** | LLM refines user questions for better search | Surfaces more relevant docs |
| **Dual Retrieval** | Search with both original and rewritten queries | Catches missed documents |
| **LLM Reranking** | Semantic reordering of results | Top results are more relevant |
| **Parallel Processing** | Multiprocessing for ingestion | Faster indexing |

---

## ğŸ“Š Evaluation Metrics

### Retrieval Metrics
- **MRR (Mean Reciprocal Rank)**: How quickly correct docs appear
- **nDCG (Normalized DCG)**: Quality of ranking
- **Keyword Coverage**: Percentage of query terms in results

### Answer Quality Metrics (LLM-as-Judge, 1-5 scale)
- **Accuracy**: Factual correctness
- **Completeness**: Coverage of all aspects
- **Relevance**: How well it addresses the question

---

## ğŸ“š Additional Documentation

### ğŸ¯ Start Here
- ğŸ“– [**Complete Workflow Guide**](docs/workflow_guide.md) - Step-by-step instructions
- ğŸ—ºï¸ [**Complete Journey**](docs/complete_journey.md) - Visual roadmap with timeline and costs

### ğŸ—ï¸ Architecture
- ğŸ›ï¸ [**System Architecture**](docs/architecture.md) - **Complete system overview with all flow diagrams**
- ğŸ“Š [**Architecture Comparison**](docs/architecture_comparison.md) - Side-by-side comparison with metrics
- ğŸ”§ [**Basic RAG Architecture**](docs/basic_rag_architecture.md) - Detailed basic RAG flow
- âš¡ [**Advanced RAG Architecture**](docs/advanced_rag_architecture.md) - Detailed advanced RAG flow

### ğŸ“– Analysis & Reference
- ğŸ” [**Detailed Comparison**](docs/rag_vs_pro.md) - In-depth feature analysis
- ğŸ“‘ [**Documentation Index**](docs/README.md) - Complete guide to all documentation
- ğŸ“ˆ **Evaluation Results** - Run `evaluator.py` to see live metrics

**Quick Navigation:**
- ğŸ†• New to RAG? â†’ [Complete Journey](docs/complete_journey.md)
- ğŸ—ï¸ Want architecture overview? â†’ [System Architecture](docs/architecture.md)
- ğŸ”¨ Ready to build? â†’ [Workflow Guide](docs/workflow_guide.md)
- ğŸ“š Need specific info? â†’ [Documentation Index](docs/README.md)

---

## ğŸ”„ RAG Pipeline Fundamentals

RAG has two main phases that run at different times:

The typical flow for building and running a Retrieval-Augmented Generation (RAG) pipeline (local or cloud, using open-source tools like LangChain / LlamaIndex / Haystack / LangGraph + Mistral 7B or similar models).

- Offline / Indexing Pipeline (done once or periodically when data changes)
- Online / Query-Time Pipeline (runs every time a user asks something)

## 1. Indexing Pipeline (Data â†’ Vector Store)
This prepares your knowledge base.
```
[Raw Data Sources]
   â†“
[1. Ingestion / Loading]
   â†“
[2. Pre-processing & Cleaning]
   â†“
[3. Chunking / Splitting]
   â†“
[4. Embedding Generation]
   â†“
[5. Metadata Enrichment (optional but very useful)]
   â†“
[6. Vector Store / Index Creation + Upsert]
```


## 2. Query-Time / Inference Pipeline (User Question â†’ Answer)
   
```
User Question
   â†“
[1. Query Pre-processing (optional: rewrite / decompose)]
   â†“
[2. Query Embedding]  â† same embedding model as indexing
   â†“
[3. Retrieval]
      â”œâ”€â†’ Vector similarity search (top-k)
      â”œâ”€â†’ Optional: Hybrid (BM25 + vector)
      â”œâ”€â†’ Optional: Re-ranking (Cohere Rerank, bge-reranker, flashrank)
      â””â”€â†’ Optional: Query transformation (HyDE, multi-query)
   â†“
[4. Context Assembly / Post-retrieval]
      â†“ (merge chunks, compress context if too long, add citations)
   â†“
[5. Prompt Construction]
      â†“ (system prompt + retrieved context + user question)
   â†“
[6. LLM Generation]  â† Mistral 7B, Llama-3.1 8B, Qwen 2.5, etc.
   â†“
[7. Optional: Post-processing]
      â†“ (guardrails, citation check, self-reflection / corrective RAG)
   â†“
Final Answer (+ sources / confidence)
```

## Combined High-Level Flow Diagram (Text Version) :

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Indexing Pipeline   â”‚               (run once / on update)
                â”‚   (Offline / Batch)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                             â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Raw Documents    â”‚           â”‚ Vector Database  â”‚
   â”‚ PDFs, Markdown,  â”‚           â”‚ (Chroma / Qdrant)â”‚
   â”‚ Web, DB, etc.    â”‚â—„â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚               â–²
            â–¼              â”‚               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   Retrieval   â”‚
   â”‚ Load â†’ Clean     â”‚    â”‚               â”‚
   â”‚ â†’ Chunk â†’ Embed  â”œâ”€â”€â”€â”€â”˜               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                           â”‚
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚   Query-Time Pipeline       â”‚
                             â”‚      (Online / Real-time)   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                 User Question
                                           â†“
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚ Embed query â†’ Retrieve â†’    â”‚
                             â”‚ Rerank â†’ Build prompt â†’ LLM â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â†“
                                      Grounded Answer
```

## Quick Start Stack Recommendation (2026, local GPU, Mistral 7B)

- Framework â†’ LlamaIndex or LangChain + LangGraph (for more advanced flows)
- Embedding â†’ bge-m3 or snowflake-arctic-embed-m (fast & good quality)
- Vector DB â†’ Chroma or Qdrant (persistent mode)
- LLM â†’ Mistral-7B-Instruct-v0.3 (or Nemo 12B / Llama-3.2 3B for faster speed)
- Reranker (strongly recommended) â†’ bge-reranker-large or flashrank
- Local inference â†’ Ollama / vLLM / llama.cpp

## Advanced Additions (Production 2026)

- Parent-document retrieval / hierarchical indexing
- Self-RAG / Corrective RAG / Adaptive RAG
- GraphRAG (entities + knowledge graph)
- Agentic flows (query routing, fallback to web search)
- Evaluation loop (RAGAS, TruLens, DeepEval)

---

## ğŸ¤ Contributing

This project is for educational purposes, demonstrating RAG evolution. Feel free to experiment with different chunking strategies, embedding models, or retrieval techniques.

## ğŸ“ License

MIT License - See LICENSE file for details
